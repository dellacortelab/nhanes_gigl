{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiYeP4EtKpJndxUCxkOWR4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dellacortelab/nhanes_gigl/blob/main/Dietary_GI_GL_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's learn how to add Dietary GI variables to a subset of the NHANES national health survey dataset.\n",
        "\n",
        "1. We will load the dataset.\n",
        "2. We will assign GI values.\n",
        "3. We calculate GL values.\n",
        "4. We aggregate the data get dietary GL values.\n",
        "5. We calculate dietary GI values.\n",
        "6. We perform some basic reporting."
      ],
      "metadata": {
        "id": "NYio3TCm8yCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQeJ-NLD8uVO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/dellacortelab/nhanes_gigl.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('./nhanes_gigl/demo_data.pkl', 'rb') as f:\n",
        "  df = pickle.load(f)"
      ],
      "metadata": {
        "id": "aSIojVTzKB4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the Data Frame."
      ],
      "metadata": {
        "id": "aWrRl-XLMF-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "HCYejOOcLuV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the unique food codes."
      ],
      "metadata": {
        "id": "DqCGzFGxMOi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food_codes = df['USDA food code'].unique().astype(int)\n",
        "print('Example food code 1:', food_codes[0],'\\nExmaple food code 2:', food_codes[1])\n"
      ],
      "metadata": {
        "id": "rxlCl2vPMD38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now see if you can find the description of the food on: https://fdc.nal.usda.gov/"
      ],
      "metadata": {
        "id": "jH0dG7BhM6vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's find the GI values associated with these food: https://glycemicindex.com/gi-search/"
      ],
      "metadata": {
        "id": "4Xu3YVT5NO_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you would have to do this for over 10k food codes by hand! Enter the AI!"
      ],
      "metadata": {
        "id": "O7m5WJlDNhQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Initialize the BERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the food descriptions\n",
        "food1 = \"Food Description from FoodData Central\"\n",
        "food2 = \"Food Description from GI Database\"\n",
        "\n",
        "# Tokenize and encode the food descriptions\n",
        "inputs1 = tokenizer(food1, return_tensors='pt', truncation=True, padding=True)\n",
        "inputs2 = tokenizer(food2, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "# Get the embeddings for the food descriptions\n",
        "with torch.no_grad():\n",
        "    embed1 = model(**inputs1).last_hidden_state.mean(dim=1)\n",
        "    embed2 = model(**inputs2).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "similarity = 1 - cosine(embed1.numpy()[0,:], embed2.numpy()[0,:])\n",
        "\n",
        "print(f\"The cosine similarity between the food descriptions is {similarity}\")"
      ],
      "metadata": {
        "id": "LaIt9NLINNc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above could be used to compare thousands of food descriptions between different databases. It uses a somewhat aged AI, called BERT. Our paper uses a more modern Large Language Model from Open-AI (the makers of ChatGPT). But for our purposes this example suffices."
      ],
      "metadata": {
        "id": "23kVc_pVOnLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume that we used the AI to align all food codes with corresponding GI values. Here, we will take random numbers for sake of time."
      ],
      "metadata": {
        "id": "4nFJS63xPA_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "gi_values = [np.random.randint(45,100) for food_code in food_codes]\n",
        "food_codes_to_gi = dict(zip(food_codes, gi_values))\n",
        "for key in list(food_codes_to_gi.keys())[:5]:\n",
        "  print(key,':', food_codes_to_gi[key])"
      ],
      "metadata": {
        "id": "H7JirX42MoJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's add the new GI variable to our dataframe!"
      ],
      "metadata": {
        "id": "OnKyILClPj3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['GI'] = df['USDA food code'].map(food_codes_to_gi)\n",
        "df"
      ],
      "metadata": {
        "id": "Kl2RXVaDPRcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate the GL value for each meal. This needs to take into account the available carbohydrates."
      ],
      "metadata": {
        "id": "bLc4RY80Pz2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Available Carbohydrate'] = df['Carbohydrate (gm)'] - df['Dietary fiber (gm)']\n",
        "df['GL'] = df['GI'] * df['Available Carbohydrate'] / 100\n",
        "df"
      ],
      "metadata": {
        "id": "vpocuJgpPxQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can sum up all the food items consumed by a given respondent in the NHANES survey. If we do this for each day of reporting, we will get Dietary GL."
      ],
      "metadata": {
        "id": "VSWaGhEWQZ6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_dict = {\n",
        "    'Dietary 20 Year Weight': 'first',\n",
        "    'Energy (kcal)': 'sum',\n",
        "    'Data release cycle': 'first',\n",
        "    'Available Carbohydrate':'sum',\n",
        "    'GI': 'sum',\n",
        "    'GL': 'sum'\n",
        "}\n",
        "summed_df = df.groupby(['respondent sequence number','Day']).agg(agg_dict).reset_index()\n",
        "summed_df.rename(columns={'GL': 'Dietary GL'}, inplace=True)\n",
        "summed_df['Dietary GI'] = summed_df['Dietary GL'] / summed_df['Available Carbohydrate']*100\n",
        "summed_df\n"
      ],
      "metadata": {
        "id": "UHZwVfq8QQuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to average over the different days, for those who respondent more than once."
      ],
      "metadata": {
        "id": "P8a5Z1ojR48e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_dict = {\n",
        "    'Dietary 20 Year Weight': 'mean',\n",
        "    'Energy (kcal)': 'mean',\n",
        "    'Available Carbohydrate':'mean',\n",
        "    'Dietary GI': 'mean',\n",
        "    'Dietary GL': 'mean',\n",
        "    'Data release cycle': 'first',\n",
        "\n",
        "}\n",
        "summed_df = summed_df.groupby(['respondent sequence number']).agg(agg_dict).reset_index()\n",
        "summed_df"
      ],
      "metadata": {
        "id": "9qg0P6WURrfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we could do some exploration of the participants. In the example data, we only have Data release cycle as a variable. Others are available from NHANES and were used in the paper. Here, however, we just focus on time trends. Let's plot the average values and approximate the errors."
      ],
      "metadata": {
        "id": "ZaBbKgJnTPl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a plot showing means of df['Dietary GL'] aggregated by column Data release cycle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.pointplot(x='Data release cycle', y='Dietary GL', data=summed_df)\n",
        "plt.xlabel('Data Release Cycle')\n",
        "plt.ylabel('Dietary GL')\n",
        "plt.title('Dietary GL by Data Release Cycle')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.pointplot(x='Data release cycle', y='Dietary GI', data=summed_df)\n",
        "plt.xlabel('Data Release Cycle')\n",
        "plt.ylabel('Dietary GI')\n",
        "plt.title('Dietary GI by Data Release Cycle')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9HrpnH7zTr6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the plots. Do you see any interesting trends? Hopefully not, we drew random variables for GI!\n",
        "\n",
        "Before these values could be interpreted, some important things need to be done.\n",
        "\n",
        "1. The Dietary GL variable should be \"adjusted\" for total energy intake according to the residual method.\n",
        "2. We also need to exclude unreasonbale consumption levels.\n",
        "3. We want to use the NHANES weights to correctly calculate means."
      ],
      "metadata": {
        "id": "iE5qWEYrIGHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now we need to energy adjust!\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "for var in ['Dietary GI','Dietary GL', 'Available Carbohydrate']:\n",
        "    #index to avoid nan!\n",
        "\n",
        "    idx_energy = summed_df['Energy (kcal)'].notna()\n",
        "    idx = summed_df['Energy (kcal)'].notna() & summed_df[var].notna()\n",
        "\n",
        "    # Separate the independent variable (X) and the dependent variable (Y)\n",
        "    X, Y = summed_df['Energy (kcal)'][idx].to_numpy().reshape(-1, 1),summed_df[var][idx].to_numpy().reshape(-1, 1)\n",
        "\n",
        "    # Create a linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Fit the model to the data\n",
        "    model.fit(X, Y)\n",
        "\n",
        "    # Get the slope (m) and y-intercept (b)\n",
        "    slope = model.coef_[0]\n",
        "    intercept = model.intercept_\n",
        "\n",
        "    # Predict Y values based on the regression line\n",
        "    predicted_Y = model.predict(X)\n",
        "\n",
        "    #Avoid missing or execssive consumption levels\n",
        "    condition1 = summed_df['Energy (kcal)'] > 600\n",
        "    condition2 = summed_df['Energy (kcal)'] < 5000\n",
        "\n",
        "    baseline_df = summed_df[(condition1 & condition2)]\n",
        "\n",
        "    mean_kcal = (baseline_df['Energy (kcal)'] * baseline_df['Dietary 20 Year Weight']).sum() / baseline_df['Dietary 20 Year Weight'].sum()\n",
        "    print('Baseline Energy intake mean', mean_kcal)\n",
        "    baseline = model.predict(np.array([mean_kcal]).reshape(-1,1))[0][0]\n",
        "    print(var, baseline)\n",
        "    # Add the new variable\n",
        "    summed_df['Adjusted '+var] = pd.NA\n",
        "\n",
        "    summed_df.loc[idx, 'Adjusted '+var] = (Y - predicted_Y) + baseline\n",
        "\n",
        "summed_df\n"
      ],
      "metadata": {
        "id": "VVWLWB5qI4Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset could be used to calculate linear regressions. However, NHANES follows a complex sample design, requiring more involved error calculations. Details can be found here: https://wwwn.cdc.gov/nchs/nhanes/tutorials/varianceestimation.aspx .\n",
        "Unfortunately, Python does not provide the right implementation to do this calculation (and our example dataset omits the necessary PSU information), but in R, Strata, or SAS this can be easily done."
      ],
      "metadata": {
        "id": "eh9AufFiKX6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last quick check we can do is a correlation analysis. For this we can scatter two variables."
      ],
      "metadata": {
        "id": "gjrPNSFDLpFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the variables of interest\n",
        "x_var = 'Energy (kcal)'\n",
        "y_var = 'Adjusted Dietary GL'\n",
        "\n",
        "#plot\n",
        "plt.scatter(summed_df[x_var], summed_df[y_var])\n",
        "plt.xlabel(x_var)\n",
        "plt.ylabel(y_var)\n",
        "plt.title('Rough Correlation Analysis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "65a3g7SRJgZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which two variable do you expect to be highly correlated? Test a few combinations and check your intuition."
      ],
      "metadata": {
        "id": "EV9TxDgiMBe8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVEpcXx4L8Va"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}